import speech_recognition as sr
r=sr.Recognizer()  
with sr.Microphone() as source:
    
    print("SAY1");
    audio1=r.listen(source)
    print("text1  :  "+r.recognize_google(audio1,language="hi-IN"));
    k=r.recognize_google(audio1,language="hi-IN");
    print(" ");
    
try:
    c1=r.recognize_google(audio1)
except:
    pass;
print(type(k))
print(k)
from google_trans_new import google_translator  
translator = google_translator() 
print(type(k))
print(k)
translate_text = translator.translate('मुझे 21 अप्रैल को दिल्ली से चेन्नई जाना है जनरल कंपार्टमेंट में',lang_tgt='en')  
print(translate_text)
translate_text = translator.translate(k,lang_tgt='en')  
print(translate_text)
import nltk
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import pyplot
df=pd.read_csv("Dataset.csv")
df.head(10)
df.info()
text = translate_text
tokens = nltk.word_tokenize(text)
print(tokens)
tag = nltk.pos_tag(tokens)
print(tag)
grammar = "NP: {<TO>*<NNP>*<IN>*<NNP>*<CD>*}"
cp  =nltk.RegexpParser(grammar)
result = cp.parse(tag)
print(result)
result.draw() 
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize(text))
import nltk
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

sentence = translate_text
punctuations="?:!.,;"
sentence_words = nltk.word_tokenize(sentence)
for word in sentence_words:
    if word in punctuations:
        sentence_words.remove(word)

sentence_words
print("{0:20}{1:20}".format("Word","Lemma"))
for word in sentence_words:
    print ("{0:20}{1:20}".format(word,wordnet_lemmatizer.lemmatize(word)))

def non_ascii(text):
    return"".join(i for i in text if ord(i)<128)
def stop_words(text):
    text=text.split()
    stops=set(stopwords.words("english"))
    text=[w for w in text if not w in stops]
    text=" ".join(text)
    return text
df['new_text']=df['text'].apply(non_ascii)
df['new_text']=df.new_text.apply(func=stop_words)
databank=[]
for words in df['new_text']:
    databank.append(words.split())
pretrained_model=Word2Vec(vector_size=300,window=5,min_count=2)
pretrained_model.build_vocab(databank)
pretrained_model.train(databank, total_examples=pretrained_model.corpus_count, epochs =5)
pretrained_model.wv.most_similar(positive=["Delhi"])
bigram_transformer = Phrases(databank)
model = Word2Vec(bigram_transformer[databank], min_count=1)
model
cv=CountVectorizer(max_features=5000,ngram_range=(1,3))
x=cv.fit_transform(databank).toarray()
pickle.dump(cv,open('cv.pkl',wb))
y=message['from']
model = Word2Vec(min_count=1)
model.build_vocab(sentences)  # prepare the model vocabulary
model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33, random_state=40)
